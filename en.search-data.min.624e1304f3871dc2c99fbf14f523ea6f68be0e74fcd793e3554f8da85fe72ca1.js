'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/docs/go/1.go%E7%AE%80%E4%BB%8B/1.1.hello-world/','title':"1.1.hello-world",'content':"1.1.hello-world 基本程序结构 touch hello_world.go package main //包,表名代码所在的模块(包) import \u0026quot;fmt\u0026quot;\t// 引入代码依赖 // 功能实现 func main() { fmt.Println(\u0026quot;Hello World\u0026quot;) } 应用程序入口  1.必须是main包: package main 2.必须是main方法: func main() 3.文件名不一定是main.go 包名和文件路径是没关系的  退出返回值  go中main函数不支持任何返回值 通过os.Exit来返回状态  获取命令行参数  main函数不支持传入参数 在程序中直接通过os.Args获取命令行参数  "});index.add({'id':1,'href':'/docs/go/1.go%E7%AE%80%E4%BB%8B/','title':"1.go简介",'content':"1.go简介 "});index.add({'id':2,'href':'/docs/algorithm/%E5%9B%BE/1.%E5%9B%BE%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86/','title':"1.「图」的基本知识",'content':"1.「图」的基本知识 「图」大概是最接近生活的一种数据结构了，生活中各种关系图便是「图」最真实的写照。比如，我们每个人的朋友交际圈就是一个巨大的「图」。  图1.小A的朋友交际图    「图」的类型 「图」的类型有很多,我们将介绍三种类型的图：无向图、有向图、加权图。\n无向图 「无向图」的图中任意两个顶点之间的边都是没有方向的。 「图1. 小A的朋友交际图」是一个无向图。\n有向图 「有向图」的图中任意两个顶点之间的边都是有方向的。 「图 2. 有向图的示例图」是一个有向图。  图2.有向图的示例图    加权图 「加权图」的图中的每条边都带有一个相关的权重。这里的权重可以是任何一种度量，比如时间，距离，尺寸等。生活中最常见的「加权图」应该就是我们的地图了。在下方的「图 3. 加权图的示例图」中，每条边上都标有距离，它们可以视为每个边上的权重。  图3.加权图的示例图    「图」的定义和相关术语 「图」是由顶点和边组成的一种非线形数据结构。在「图」中有很多相关术语来描述一个图。如果在后面的学习中遇到不认识的术语，请回到这里查看相关的定义。\n 顶点：在「图 1. 小A的朋友交际图」中，小 A 、小 B 、小 C 等均称为「图」的顶点。 边：顶点之间的连接线称为边。在「图 1. 小A的朋友交际图」中，小 A 和小 B 之间的连接线就是图中的一条边。 路径：从一个顶点到另一个顶点之间经过的所有顶点的集合。在「图 1. 小A的朋友交际图」中，从小 A 到小 C 的路径为[小A, 小B, 小C] 或者[小A, 小G, 小B, 小C]或者[小A, 小E, 小F, 小D, 小B, 小C]。 **注意：**两个顶点之间的路径可以是很多条。 路径长度：一条路径上经过的边的数量。在「图 1. 小A的朋友交际图」中，从小 A 到小 C 的路径长度为2或者3或者5。 环：起点和终点为同一个顶点的路径。在「图 1. 小A的朋友交际图」中，[小A, 小B, 小D, 小F, 小E]组成了一个环。同理，[小A, 小G, 小B]也组成了一个环。 负权环：在「加权图」中，如果一个环的所有边的权重加起来为负数，我们就称之为「负权环」。在「图4. 负权环」中，它的所有边的权重和为-3。 连通性：两个不同顶点之间存在至少一条路径，则称这两个顶点是连通的。在「图 1. 小A的朋友交际图」中，小 A 和小 C 是连通的，因为它们之间至少有一条路径。 顶点的度：「度」适用于无向图，指的是和该顶点相连接的所有边数称为顶点的度。在「图 1. 小A的朋友交际图」中，顶点小A的度为3，因为与它相连接的边有3条。 顶点的入度：「入度」适用于有向图，一个顶点的入度为dd，则表示有dd条与顶点相连的边指向该顶点。在「图 2. 有向图的示例图」中，A的入度为1，由F指向A的边。 顶点的出度：「出度」适用于有向图，它与「入度」相反。一个顶点的出度为dd，则表示有dd条与顶点相连的边以该顶点为起点。在「图 2. 有向图的示例图」中，A 的出度为3，分别为，A 指向 B 的边，A 指向 C 的边，和 A 指向 G 的边。  图4.负权环      "});index.add({'id':3,'href':'/docs/kafka/1.%E5%BC%82%E6%AD%A5%E9%80%9A%E4%BF%A1%E5%8E%9F%E7%90%86/','title':"1.异步通信原理",'content':"1.异步通信原理 观察者模式  观察者模式（Observer），又叫发布-订阅模式（Publish/Subscribe） 定义对象间一种一对多的依赖关系，使得每当一个对象改变状态，则所有依赖于它的对象都会得到 通知并自动更新。 一个对象（目标对象）的状态发生改变，所有的依赖对象（观察者对象）都将得到通知。   现实生活中的应用场景  京东到货通知    生产者消费者模式  传统模式  生产者直接将消息传递给指定的消费者 耦合性特别高，当生产者或者消费者发生变化，都需要重写业务逻辑   生产者消费者模式  通过一个容器来解决生产者和消费者的强耦合问题。生产者和消费者彼此之间不直接通讯，而通过阻塞队列来进行通讯     数据传递流程  生产者消费者模式，即N个线程进行生产，同时N个线程进行消费，两种角色通过内存缓冲区 进行通信， 生产者负责向缓冲区里面添加数据单元 消费者负责从缓冲区里面取出数据单元  一般遵循先进先出的原则      缓冲区  解耦  假设生产者和消费者分别是两个类。如果让生产者直接调用消费者的某个方法，那么生产者 对于消费者就会产生依赖   支持并发  生产者直接调用消费者的某个方法过程中函数调用是同步的 万一消费者处理数据很慢，生产者就会白白糟蹋大好时光    数据单元  关联到业务对象  数据单元必须关联到某种业务对象   完整性  就是在传输过程中，要保证该数据单元的完整   独立性  就是各个数据单元之间没有互相依赖 某个数据单元传输失败不应该影响已经完成传输的单元；也不应该影响尚未传输的单元。   颗粒度  数据单元需要关联到某种业务对象。那么数据单元和业务对象应该处于的关系（一对一？一 对多） 如果颗粒度过小会增加数据传输的次数 如果颗粒度过大会增加单个数据传输的时间，影响后期消费    "});index.add({'id':4,'href':'/docs/algorithm/%E5%9B%BE/2.%E5%B9%B6%E6%9F%A5%E9%9B%86/2.1.quick-find-%E7%9A%84%E5%B9%B6%E6%9F%A5%E9%9B%86/','title':"2.1.Quick Find 的「并查集」",'content':"2.1.Quick Find 的「并查集」 Quick Find的工作原理 //todo::\n伪代码 (todo:://提供一版go的代码) // UnionFind.class public class UnionFind { int root[]; public UnionFind(int size) { root = new int[size]; for (int i = 0; i \u0026lt; size; i++) { root[i] = i; } } public int find(int x) { return root[x]; } public void union(int x, int y) { int rootX = find(x); int rootY = find(y); if (rootX != rootY) { for (int i = 0; i \u0026lt; root.length; i++) { if (root[i] == rootY) { root[i] = rootX; } } } }; public boolean connected(int x, int y) { return find(x) == find(y); } } // App.java // 测试样例 public class App { public static void main(String[] args) throws Exception { UnionFind uf = new UnionFind(10); // 1-2-5-6-7 3-8-9 4 uf.union(1, 2); uf.union(2, 5); uf.union(5, 6); uf.union(6, 7); uf.union(3, 8); uf.union(8, 9); System.out.println(uf.connected(1, 5)); // true System.out.println(uf.connected(5, 7)); // true System.out.println(uf.connected(4, 9)); // false // 1-2-5-6-7 3-8-9-4 uf.union(9, 4); System.out.println(uf.connected(4, 9)); // true } } 时间复杂度     UnionFind 构造函数 find 函数 union 函数 connected 函数     时间复杂度 O(N) O(1) O(N) O(1)    注：NN 为「图」中顶点的个数。\n"});index.add({'id':5,'href':'/docs/notices/%E6%88%90%E7%A5%9E%E4%B9%8B%E8%B7%AF/1/','title':"2021-12-22",'content':"日期:2021-12-22（第一天） 一直想改变,一直想学点东西，工作太忙，没有恒心，大多半途而废了。\n此刻: 突然感觉我不能再这样下去了，我要开始干点什么了，那就成为自己的\u0026quot;神\u0026quot;吧,起航!!!\n我要疯狂学习4个月\n"});index.add({'id':6,'href':'/docs/algorithm/%E5%9B%BE/3.%E5%9B%BE%E7%9A%84%E6%B7%B1%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95-/3.1.%E6%B7%B1%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95-%E9%81%8D%E5%8E%86%E6%89%80%E6%9C%89%E9%A1%B6%E7%82%B9/','title':"3.1.深度优先搜索算法-遍历所有顶点",'content':"3.1.深度优先搜索算法-遍历所有顶点 代码 时间复杂度 O(V+E)。\nVV 表示顶点数，EE 表示边数。\n空间复杂度 O(V)。\nVV 表示顶点数。\n"});index.add({'id':7,'href':'/docs/algorithm/%E5%9B%BE/4.%E5%9B%BE%E7%9A%84%E5%B9%BF%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95/4.1.%E5%B9%BF%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95-%E9%81%8D%E5%8E%86%E6%89%80%E6%9C%89%E9%A1%B6%E7%82%B9/','title':"4.1.广度优先搜索算法-遍历所有顶点",'content':"4.1.广度优先搜索算法-遍历所有顶点 代码 //todo::\n时间复杂度 O(V+E)O(V+E)。\nVV 表示顶点数，EE 表示边数。\n空间复杂度 O(V)O(V)。\n"});index.add({'id':8,'href':'/docs/algorithm/%E5%9B%BE/4.%E5%9B%BE%E7%9A%84%E5%B9%BF%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95/4.2.%E5%B9%BF%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95-%E6%B1%82%E4%B8%A4%E7%82%B9%E4%B9%8B%E9%97%B4%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84/','title':"4.2.广度优先搜索算法-求两点之间最短路径",'content':"4.2.广度优先搜索算法-求两点之间最短路径 代码 //todo::\n时间复杂度 O(V+E)O(V+E)。\nVV 表示顶点数，EE 表示边数。\n空间复杂度 O(V)O(V)。\nVV 表示顶点数。\n"});index.add({'id':9,'href':'/docs/algorithm/','title':"algorithm",'content':"algorithm 1: leetcode题目按照什么顺序去刷 按照一个框架来刷题目，比如说这周刷字符串,下周刷数组，按照一个章节一个章节这样来做，最好是跟着书、教程来一起做题. 做完教程的配套题目后leetcode上按频率从高到低刷,最少做每组的前20个。 做完一道题如果你比较陌生的思路， 继续做这道题的\u0026quot;相似问题\u0026rdquo;。面试题做一遍热题例如:https://leetcode-cn.com/problem-list/qg88wci/, https://leetcode-cn.com/problem-list/2cktkvj/\n参加周赛、双周赛，能做出前3个就不错了\n2: 我们需要刷多少题 400左右，看刷题质量，看到题目能默写出来\n"});index.add({'id':10,'href':'/docs/go/','title':"go",'content':"GO "});index.add({'id':11,'href':'/docs/kafka/','title':"kafka",'content':"kafka "});index.add({'id':12,'href':'/docs/algorithm/%E6%8E%92%E5%BA%8F/on2/','title':"O(n^2)",'content':"O(n^2) 本章我们介绍了三种基础排序算法：冒泡排序、选择排序、插入排序。\n冒泡排序 冒泡排序有两种优化方式：\n 记录当前轮次是否发生过交换，没有发生过交换表示数组已经有序； 记录上次发生交换的位置，下一轮排序时只比较到此位置。  选择排序 选择排序可以演变为二元选择排序：\n 二元选择排序：一次遍历选出两个值——最大值和最小值； 二元选择排序剪枝优化：当某一轮遍历出现最大值和最小值相等，表示数组中剩余元素已经全部相等。  插入排序 插入排序有两种写法：\n 交换法：新数字通过不断交换找到自己合适的位置； 移动法：旧数字不断向后移动，直到新数字找到合适的位置。  不同点 选择排序是不稳定的，冒泡排序、插入排序是稳定的；\n在这三个排序算法中，选择排序交换的次数是最少的；\n在数组几乎有序的情况下，插入排序的时间复杂度接近线性级别。\n"});index.add({'id':13,'href':'/docs/algorithm/%E6%8E%92%E5%BA%8F/on2/%E5%86%92%E6%B3%A1%E6%8E%92%E5%BA%8F/','title':"冒泡排序",'content':"冒泡排序 冒泡排序有三种写法：  一边比较一边向后两两交换，将最大值 / 最小值冒泡到最后一位； 经过优化的写法：使用一个变量记录当前轮次的比较是否发生过交换，如果没有发生交换表示已经有序，不再继续排序； 进一步优化的写法：除了使用变量记录当前轮次是否发生交换外，再使用一个变量记录上次发生交换的位置，下一轮排序时到达上次交换的位置就停止比较。  代码 package main import ( \u0026quot;fmt\u0026quot; ) func main() { values1 := []int{4, 93, 84, 85, 80, 37, 81, 93, 27, 12} fmt.Println(values1) // [4 93 84 85 80 37 81 93 27 12] MySort1(values1) fmt.Println(values1) // [4 12 27 37 80 81 84 85 93 93] values2 := []int{4, 93, 84, 85, 80, 37, 81, 93, 27, 12} fmt.Println(values2) // [4 93 84 85 80 37 81 93 27 12] MySort1(values2) fmt.Println(values2) // [4 12 27 37 80 81 84 85 93 93] values3 := []int{4, 93, 84, 85, 80, 37, 81, 93, 27, 12} fmt.Println(values3) // [4 93 84 85 80 37 81 93 27 12] MySort1(values3) fmt.Println(values3) // [4 12 27 37 80 81 84 85 93 93] } // 第一种排序 func MySort1(arr []int) { for i := 0; i \u0026lt; len(arr); i++ { for j := 0; j \u0026lt; len(arr) -i - 1; j++ { if arr[j+1] \u0026lt; arr[j] { arr[j],arr[j+1] = arr[j+1],arr[j] } } } } // 第二种排序 func MySort2(arr []int) { var swapped bool = true; for i := 0; i \u0026lt; len(arr); i++ { if !swapped { break; } swapped = false; for j := 0; j \u0026lt; len(arr) -i - 1; j++ { if arr[j+1] \u0026lt; arr[j] { arr[j],arr[j+1] = arr[j+1],arr[j] swapped = true; } } } } // 第三种排序 func MySort3(arr []int) { var swapped bool = true; var indexOfLastUnsortedElement int = len(arr) - 1; // 上次发生交换的位置 var swappedIndex int = -1; for swapped { swapped = false; for i := 0; i \u0026lt; indexOfLastUnsortedElement; i++ { if arr[i] \u0026gt; arr[i+1] { arr[i],arr[i+1] = arr[i+1],arr[i] // 表示发生了交换 swapped = true; // 更新交换的位置 swappedIndex = i; } } // 最后一个没有经过排序的元素的下标就是最后一次发生交换的位置 indexOfLastUnsortedElement = swappedIndex; } } "});index.add({'id':14,'href':'/docs/algorithm/%E5%9B%BE/','title':"图",'content':"图 "});index.add({'id':15,'href':'/docs/algorithm/%E5%A0%86/','title':"堆",'content':"堆 "});index.add({'id':16,'href':'/docs/algorithm/%E6%8E%92%E5%BA%8F/','title':"排序",'content':"排序 "});index.add({'id':17,'href':'/docs/vbox/%E9%85%8D%E7%BD%AE%E7%BD%91%E5%8D%A1/','title':"配置网卡",'content':"配置网卡 https://blog.csdn.net/weixin_42541479/article/details/118407951\n1、安装centos7的时候注意选择两个网卡 两个网卡分别为：\nnat(虚拟机访问互联网，使用10.0.2.x段)\nhost-only(虚拟机和主机互相通信，使用192.168.56.x段)\n在偏好设置里面设置网络。如下图配置：\n    2、配置网络    3、开机，进入 cd /etc/etc/sysconfig/network-scripts/ 目录 cd /etc/sysconfig/network-scripts/ ls   vi ifcfg-enp0s3   vi ifcfg-enp0s8   service network restart   4、验证    拷贝 "});index.add({'id':18,'href':'/docs/algorithm/%E5%9B%BE/2.%E5%B9%B6%E6%9F%A5%E9%9B%86/2.2.quick-union-%E7%9A%84%E5%B9%B6%E6%9F%A5%E9%9B%86/','title':"2.2.Quick Union 的「并查集」",'content':"2.2.Quick Union 的「并查集」 Quick Union 工作原理 //todo::\n为什么 Quick Union 比 Quick Find 更加高效？ 总体来说，Quick Union 是比 Quick Find 更加高效的。 //todo::\n伪代码 public class UnionFind { int root[]; public UnionFind(int size) { root = new int[size]; for (int i = 0; i \u0026lt; size; i++) { root[i] = i; } } public int find(int x) { while (x != root[x]) { x = root[x]; } return x; } public void union(int x, int y) { int rootX = find(x); int rootY = find(y); if (rootX != rootY) { root[rootY] = rootX; } }; public boolean connected(int x, int y) { return find(x) == find(y); } } // App.java // 测试样例 public class App { public static void main(String[] args) throws Exception { UnionFind uf = new UnionFind(10); // 1-2-5-6-7 3-8-9 4 uf.union(1, 2); uf.union(2, 5); uf.union(5, 6); uf.union(6, 7); uf.union(3, 8); uf.union(8, 9); System.out.println(uf.connected(1, 5)); // true System.out.println(uf.connected(5, 7)); // true System.out.println(uf.connected(4, 9)); // false // 1-2-5-6-7 3-8-9-4 uf.union(9, 4); System.out.println(uf.connected(4, 9)); // true } } 时间复杂度     UnionFind 构造函数 find 函数 union 函数 connected 函数     时间复杂度 O(N) O(H) O(H) O(H)    注：NN 为「图」中顶点的个数，HH 为「树」的高度。\n"});index.add({'id':19,'href':'/docs/go/2.%E5%9F%BA%E6%9C%AC%E7%A8%8B%E5%BA%8F%E7%BB%93%E6%9E%84/','title':"2.基本程序结构",'content':"2.基本程序结构 "});index.add({'id':20,'href':'/docs/algorithm/%E5%9B%BE/2.%E5%B9%B6%E6%9F%A5%E9%9B%86/','title':"2.并查集",'content':"2.并查集 如果给你一些顶点，并且告诉你每个顶点的连接关系，你如何才能快速的找出两个顶点是否具有连通性呢？ 如「图 5. 连通性问题」，该图给出了顶点与顶点之间的连接关系， 那么，我们如何让计算机快速定位 (0, 3) , (1, 5), (7, 8) 是否相连呢？此时我们就需要机智的「并查集」数据结构了。 很多地方也会称「并查集」为算法。  图5.连通性问题    「并查集」的主要作用是用来解决网络中的连通性。这里的「网络」可以是计算机的网络， 也可以是人际关系的网络等等。例如，你可以通过「并查集」来判定两个人是否来自同一个祖先。\n「并查集」常用术语  父节点：顶点的直接父亲节点。如「图5. 连通性问题」中，顶点 3 的父节点是 1；顶点 2 的父节点是 0；顶点 9 的父节点是自己本身 9。 根节点：没有父节点的节点，本身可以视为自己的父节点。如「图5. 连通性问题」中，顶点 3 和 2 的根节点都是 0；0 即是自己本身的父节点，也是自己的根节点；顶点 9 的根节点是自己本身 9。  「并查集」基本思想 //todo::\n「并查集」编程思想 //todo::\n「并查集」的两个重要函数  find 函数：找到给定顶点的根结点。如「图5. 连通性问题」中，find 函数需要找到顶点 3 的根结点是 0 。 union 函数：合并两个顶点，并将他们的根结点保持一致。如「图5. 连通性问题」中，如果我们需要将顶点 4 和 顶点 5 合并，那么顶点 4 和 顶点 5 至少需要保持根节点一致，也就是说 union 函数需要将顶点 4 和 顶点 5 的根结点修改为相同的根结点。  「并查集」的两个实现方式  Quick Find 实现方式：它指的是实现「并查集」时，find 函数时间复杂度很低为 O(1)O(1)，但对应的 union 函数就需要承担更多的责任，它的时间复杂度为 O(N)O(N)。 Quick Union 实现方式：它指的是实现「并查集」时，相对于 Quick Find 的实现方式，我们通过降低 union 函数的职责来提高它的效率，但同时，我们也增加了 find 函数的职责。  "});index.add({'id':21,'href':'/docs/kafka/2.%E6%B6%88%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%8E%9F%E7%90%86/','title':"2.消息系统原理",'content':"2.消息系统原理 一个消息系统负责将数据从一个应用传递到另外一个应用，应用只需关注于数据，无需关注数据在两个 或多个应用间是如何传递的。\n点对点消息传递  在点对点消息系统中，消息持久化到一个队列中。此时，将有一个或多个消费者消费队列中的数 据。但是一条消息只能被消费一次。 当一个消费者消费了队列中的某条数据之后，该条数据则从消息队列中删除。 该模式即使有多个消费者同时消费数据，也能保证数据处理的顺序。 基于推送模型的消息系统，由消息代理记录消费状态。  消息代理将消息推送(push)到消费者后，标记这条消息为已经被消费，但是这种方式无法很 好地保证消费的处理语义。      发布订阅消息传递  在发布-订阅消息系统中，消息被持久化到一个topic中。 消费者可以订阅一个或多个topic，消费者可以消费该topic中所有的数据，同一条数据可以被多个 消费者消费，数据被消费后不会立马删除。 在发布-订阅消息系统中，消息的生产者称为发布者，消费者称为订阅者。 Kafka 采取拉取模型(Poll)，由自己控制消费速度，以及消费的进度，消费者可以按照任意的偏移 量进行消费。    "});index.add({'id':22,'href':'/docs/algorithm/%E5%9B%BE/3.%E5%9B%BE%E7%9A%84%E6%B7%B1%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95-/3.2.%E6%B7%B1%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95-%E9%81%8D%E5%8E%86%E4%B8%A4%E7%82%B9%E4%B9%8B%E9%97%B4%E6%89%80%E6%9C%89%E8%B7%AF%E5%BE%84/','title':"3.2.深度优先搜索算法-遍历两点之间所有路径",'content':"3.2.深度优先搜索算法-遍历两点之间所有路径 代码   "});index.add({'id':23,'href':'/docs/algorithm/%E6%8E%92%E5%BA%8F/onlogn/','title':"O(nlogn)",'content':"O(nlogn) "});index.add({'id':24,'href':'/docs/vbox/%E6%8B%B7%E8%B4%9D/','title':"拷贝",'content':"拷贝    修改配置 /etc/sysconfig/network-scripts/ifcfg-enp0s3 /etc/sysconfig/network-scripts/ifcfg-enp0s8 注意: 参考配置网卡里面的配置\n"});index.add({'id':25,'href':'/docs/algorithm/%E6%8E%92%E5%BA%8F/on2/%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F/','title':"选择排序",'content':"选择排序 选择排序的思想是：双重循环遍历数组，每经过一轮比较，找到最小元素的下标，将其交换至首位。\npackage main import \u0026quot;fmt\u0026quot; func main() { values1 := []int{4, 93, 84, 85, 80, 37, 81, 93, 27, 12} fmt.Println(values1) // [4 93 84 85 80 37 81 93 27 12] SelectSort(values1) fmt.Println(values1) // [4 12 27 37 80 81 84 85 93 93] values2 := []int{4, 93, 84, 85, 80, 37, 81, 93, 27, 12} fmt.Println(values2) // [4 93 84 85 80 37 81 93 27 12] SelectSort(values2) fmt.Println(values2) // [4 12 27 37 80 81 84 85 93 93] } func SelectSort(values []int) { for i := 0; i \u0026lt; len(values); i++ { min := i for j := len(values) - 1; j \u0026gt; i; j-- { if values[j] \u0026lt; values[min] { min = j } } values[i], values[min] = values[min], values[i] } } func SelectSort2(values []int) { var minIndex int var maxIndex int // i 只需要遍历一半 for i := 0; i \u0026lt; len(values) / 2; i++ { minIndex = i maxIndex = i for j := i + 1; j \u0026lt; len(values) -1; j++ { if values[minIndex] \u0026gt; values[j] { // 记录最小值的下标 minIndex = j } if values[maxIndex] \u0026lt; values[j] { // 记录最大值的下标 maxIndex = j } } // 如果 minIndex 和 maxIndex 都相等，那么他们必定都等于 i，且后面的所有数字都与 arr[i] 相等，此时已经排序完成 if minIndex == maxIndex { break; } // 将最小元素交换至首位 values[minIndex], values[i] = values[i], values[minIndex] // 如果最大值的下标刚好是 i，由于 arr[i] 和 arr[minIndex] 已经交换了，所以这里要更新 maxIndex 的值。 if maxIndex == i { maxIndex = minIndex } // 将最大元素交换至末尾 lastIndex := len(values) -1 - i; values[maxIndex], values[lastIndex] = values[lastIndex], values[maxIndex] } } "});index.add({'id':26,'href':'/docs/algorithm/%E5%9B%BE/2.%E5%B9%B6%E6%9F%A5%E9%9B%86/2.3.%E6%8C%89%E7%A7%A9%E5%90%88%E5%B9%B6%E7%9A%84%E5%B9%B6%E6%9F%A5%E9%9B%86/','title':"2.3.按秩合并的「并查集」",'content':"2.3.按秩合并的「并查集」 小伙伴看到这里的时候，我们其实已经实现了 2 种「并查集」。但它们都有一个很大的缺点，这个缺点就是通过 union 函数连接顶点之后，可能所有顶点连成一条线形成「图 5. 一条线的图」，这就是我们 find 函数在最坏的情况下的样子。那么我们有办法解决吗？\n当然，伟大的科学家已经给出了解决方案，就是按秩合并。这里的「秩」可以理解为「秩序」。之前我们在 union 的时候，我们是随机选择 x 和 y 中的一个根节点/父节点作为另一个顶点的根节点。但是在「按秩合并」中，我们是按照「某种秩序」选择一个父节点。\n这里的「秩」指的是每个顶点所处的高度。我们每次 union 两个顶点的时候，选择根节点的时候不是随机的选择某个顶点的根节点，而是将「秩」大的那个根节点作为两个顶点的根节点，换句话说，我们将低的树合并到高的树之下，将高的树的根节点作为两个顶点的根节点。这样，我们就避免了所有的顶点连成一条线，这就是按秩合并优化的「并查集」。  图5.一条线的图    伪代码 // UnionFind.class public class UnionFind { int root[]; int rank[]; public UnionFind(int size) { root = new int[size]; rank = new int[size]; for (int i = 0; i \u0026lt; size; i++) { root[i] = i; rank[i] = 1; } } public int find(int x) { while (x != root[x]) { x = root[x]; } return x; } public void union(int x, int y) { int rootX = find(x); int rootY = find(y); if (rootX != rootY) { if (rank[rootX] \u0026gt; rank[rootY]) { root[rootY] = rootX; } else if (rank[rootX] \u0026lt; rank[rootY]) { root[rootX] = rootY; } else { root[rootY] = rootX; rank[rootX] += 1; } } }; public boolean connected(int x, int y) { return find(x) == find(y); } } // App.java // 测试样例 public class App { public static void main(String[] args) throws Exception { UnionFind uf = new UnionFind(10); // 1-2-5-6-7 3-8-9 4 uf.union(1, 2); uf.union(2, 5); uf.union(5, 6); uf.union(6, 7); uf.union(3, 8); uf.union(8, 9); System.out.println(uf.connected(1, 5)); // true System.out.println(uf.connected(5, 7)); // true System.out.println(uf.connected(4, 9)); // false // 1-2-5-6-7 3-8-9-4 uf.union(9, 4); System.out.println(uf.connected(4, 9)); // true } } 时间复杂度     UnionFind 构造函数 find 函数 union 函数 connected 函数     时间复杂度 O(N) O(logN) O(logN) O(logN)    注：N 为「图」中顶点的个数。\n"});index.add({'id':27,'href':'/docs/kafka/3.kafka%E7%AE%80%E4%BB%8B/','title':"3.kafka简介",'content':"kafka简介   官网： http://kafka.apache.org\n  Kafka是由Apache软件基金会开发的一个开源流处理平台，由Scala和Java编写。Kafka是一种高吞 吐量的分布式发布订阅消息系统，它可以处理消费者在网站中的所有动作流数据。    设计目标  以时间复杂度为O(1)的方式提供消息持久化能力，即使对TB级以上数据也能保证常数时间的访问 性能。 高吞吐率。即使在非常廉价的商用机器上也能做到单机支持每秒100K条消息的传输。 支持Kafka Server间的消息分区，及分布式消费，同时保证每个partition内的消息顺序传输。 同时支持离线数据处理和实时数据处理。 支持在线水平扩展  Kafka的优点   解耦\n在项目启动之初来预测将来项目会碰到什么需求，是极其困难的。消息系统在处理过程中间插入了一个 隐含的、基于数据的接口层，两边的处理过程都要实现这一接口。这允许你独立的扩展或修改两边的处 理过程，只要确保它们遵守同样的接口约束。\n  冗余\n有些情况下，处理数据的过程会失败。除非数据被持久化，否则将造成丢失。消息队列把数据进行持久 化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。许多消息队列所采用的\u0026quot;插入-获取-删 除\u0026quot;范式中，在把一个消息从队列中删除之前，需要你的处理系统明确的指出该消息已经被处理完毕， 从而确保你的数据被安全的保存直到你使用完毕。\n  扩展性\n因为消息队列解耦了你的处理过程，所以增大消息入队和处理的频率是很容易的，只要另外增加处理过 程即可。不需要改变代码、不需要调节参数。扩展就像调大电力按钮一样简单。\n  灵活性\u0026amp;峰值处理能力\n在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见；如果为以能处理 这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的 访问压力，而不会因为突发的超负荷的请求而完全崩溃。\n  可恢复性\n系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理 消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。\n  顺序保证\n在大多使用场景下，数据处理的顺序都很重要。大部分消息队列本来就是排序的，并且能保证数据会按 照特定的顺序来处理。Kafka保证一个Partition内的消息的有序性。\n  缓冲\n在任何重要的系统中，都会有需要不同的处理时间的元素。例如，加载一张图片比应用过滤器花费更少 的时间。消息队列通过一个缓冲层来帮助任务最高效率的执行———写入队列的处理会尽可能的快速。 该缓冲有助于控制和优化数据流经过系统的速度。\n  异步通信 很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入 队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。\n  "});index.add({'id':28,'href':'/docs/algorithm/%E5%9B%BE/3.%E5%9B%BE%E7%9A%84%E6%B7%B1%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95-/','title':"3.「图」的深度优先搜索算法",'content':"3.「图」的深度优先搜索算法 在前面的「并查集」数据结构中，大家已经知道如何检查两个顶点之间的连通性。那如果给你一个「图」，你该如何找出它所有的顶点呢？以及你又如何找出它两个顶点之间的所有路径呢？此时，「深度优先搜索」算法就可以登场了。如「图6. 无向图」所示，它的所有顶点分别为[A, C, D, B, E]。给定顶点 A 和 B， 它们之间有两条路径，一条路径为[A, C, D, B]，另一条路径为[A, E, B]。  图6.无向图    「深度优先搜索」（又称「Depth First Search」，简称「DFS」）算法在「图」中主要用途：\n 遍历「图」中所有顶点； 遍历「图」中任意两点之间的所有路径。  "});index.add({'id':29,'href':'/docs/go/3.%E5%B8%B8%E7%94%A8%E9%9B%86%E5%90%88/','title':"3.常用结合",'content':"3.常用集合 "});index.add({'id':30,'href':'/docs/algorithm/%E6%8E%92%E5%BA%8F/on/','title':"O(n)",'content':"O(n) "});index.add({'id':31,'href':'/docs/algorithm/%E6%8E%92%E5%BA%8F/on2/%E6%8F%92%E5%85%A5%E6%8E%92%E5%BA%8F/','title':"插入排序",'content':"插入排序 插入排序有两种写法：  交换法：在新数字插入过程中，不断与前面的数字交换，直到找到自己合适的位置。 移动法：在新数字插入过程中，与前面的数字不断比较，前面的数字不断向后挪出位置，当新数字找到自己的位置后，插入一次即可。  "});index.add({'id':32,'href':'/docs/algorithm/%E5%9B%BE/2.%E5%B9%B6%E6%9F%A5%E9%9B%86/2.4.%E8%B7%AF%E5%BE%84%E5%8E%8B%E7%BC%A9%E4%BC%98%E5%8C%96%E7%9A%84%E5%B9%B6%E6%9F%A5%E9%9B%86/','title':"2.4.路径压缩优化的「并查集」",'content':"2.4.路径压缩优化的「并查集」 从前面的「并查集」实现方式中，我们不难看出，要想找到一个元素的根节点，需要沿着它的父亲节点的足迹一直遍历下去，直到找到它的根节点为止。如果下次再查找同一个元素的根节点，我们还是要做相同的操作。那我们有没有什么办法将它升级优化下呢？\n答案是可以的！如果我们在找到根节点之后，将所有遍历过的元素的父节点都改成根节点，那么我们下次再查询到相同元素的时候，我们就仅仅只需要遍历两个元素就可以找到它的根节点了，这是非常高效的实现方式。那么问题来了，我们如何将所有遍历过的元素的父节点都改成根节点呢？这里就要拿出「递归」算法了。这种优化我们称之为「路径压缩」优化，它是对 find 函数的一种优化。\n伪代码 // UnionFind.class public class UnionFind { int root[]; public UnionFind(int size) { root = new int[size]; for (int i = 0; i \u0026lt; size; i++) { root[i] = i; } } public int find(int x) { if (x == root[x]) { return x; } return root[x] = find(root[x]); } public void union(int x, int y) { int rootX = find(x); int rootY = find(y); if (rootX != rootY) { root[rootY] = rootX; } }; public boolean connected(int x, int y) { return find(x) == find(y); } } // App.java // 测试样例 public class App { public static void main(String[] args) throws Exception { UnionFind uf = new UnionFind(10); // 1-2-5-6-7 3-8-9 4 uf.union(1, 2); uf.union(2, 5); uf.union(5, 6); uf.union(6, 7); uf.union(3, 8); uf.union(8, 9); System.out.println(uf.connected(1, 5)); // true System.out.println(uf.connected(5, 7)); // true System.out.println(uf.connected(4, 9)); // false // 1-2-5-6-7 3-8-9-4 uf.union(9, 4); System.out.println(uf.connected(4, 9)); // true } } 时间复杂度     UnionFind 构造函数 find 函数 union 函数 connected 函数     时间复杂度 O(N) O(logN) O(logN) O(logN)    注：N 为「图」中顶点的个数。\n"});index.add({'id':33,'href':'/docs/kafka/4.kafka%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/','title':"4.kafka系统架构",'content':"4.kafka系统架构  Broker  Kafka 集群包含一个或多个服务器，服务器节点称为broker。  Topic (类似表)  每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic。 类似于数据库的表名或者ES的Index 物理上不同Topic的消息分开存储 逻辑上一个Topic的消息虽然保存于一个或多个broker上但用户只需指定消息的Topic即可生产或消 费数据而不必关心数据存于何处） 创建流程  1.controller在ZooKeeper的/brokers/topics节点上注册watcher，当topic被创建， 则 controller会通过watch得到该topic的partition/replica分配。 2.controller从/brokers/ids读取当前所有可用的broker列表，对于set_p中的每一个 partition： 2.1从分配给该partition的所有replica（称为AR）中任选一个可用的broker作为新的 leader， 并将AR设置为新的ISR 2.2将新的leader和ISR写 入/brokers/topics/[topic]/partitions/[partition]/state 3.controller通过RPC向相关的broker发送LeaderAndISRRequest。  删除流程  1.controller在zooKeeper的/brokers/topics节点上注册watcher，当topic被删除， 则controller会通过watch得到该topic的partition/replica分配。 2.若delete.topic.enable=false，结束；否则controller注册在/admin/delete_topics上 的watch被fire, controller通过回调向对应的broker发送StopReplicaRequest。 Partition (分区表的)   topic中的数据分割为一个或多个partition。 每个topic至少有一个partition,当生产者产生数据的时候，根据分配策略,选择分区,然后将消息追 加到指定的分区的末尾（队列）  ## Partation数据路由规则 1. 指定了 patition，则直接使用； 2. 未指定 patition 但指定 key，通过对 key 的 value 进行hash 选出一个 patition 3. patition 和 key 都未指定，使用轮询选出一个 patition。  每条消息都会有一个自增的编号  标识顺序 用于标识消息的偏移量   每个partition中的数据使用多个segment文件存储。 partition中的数据是有序的，不同partition间的数据丢失了数据的顺序。 如果topic有多个partition，消费数据时就不能保证数据的顺序。严格保证消息的消费顺序的场景 下，需要将partition数目设为1。  Leader (读写) 每个partition有多个副本，其中有且仅有一个作为Leader，Leader是当前负责数据的读写的 partition。\n1. producer 先从 zookeeper 的 \u0026quot;/brokers/.../state\u0026quot; 节点找到该 partition 的 leader 2. producer 将消息发送给该 leader 3. leader 将消息写入本地 log 4. followers 从 leader pull 消息，写入本地 log 后 leader 发送 ACK 5. leader 收到所有 ISR 中的 replica 的 ACK 后，增加 HW（high watermark， 最后 commit 的 offset） 并向 producer 发送 ACK Follower (只备份)  Follower跟随Leader，所有写请求都通过Leader路由，数据变更会广播给所有Follower， Follower与Leader保持数据同步。 如果Leader失效，则从Follower中选举出一个新的Leader。 当Follower挂掉、卡住或者同步太慢，leader会把这个follower从“in sync replicas”（ISR）列表中 删除，重新创建一个Follower。  replication (Follower的另外一个叫法)  数据会存放到topic的partation中，但是有可能分区会损坏 我们需要对分区的数据进行备份（备份多少取决于你对数据的重视程度） 我们将分区的分为Leader(1)和Follower(N)  Leader负责写入和读取数据 Follower只负责备份 保证了数据的一致性   备份数设置为N，表示主+备=N(参考HDFS)  ## Kafka 分配 Replica 的算法如下 1. 将所有 broker（假设共 n 个 broker）和待分配的 partition 排序 2. 将第 i 个 partition 分配到第（i mod n）个 broker 上 3. 将第 i 个 partition 的第 j 个 replica 分配到第（(i + j) mode n）个 broker 上   producer  生产者即数据的发布者，该角色将消息发布到Kafka的topic中。 broker接收到生产者发送的消息后，broker将该消息追加到当前用于追加数据的segment文件 中。 生产者发送的消息，存储到一个partition中，生产者也可以指定数据存储的partition。  consumer  消费者可以从broker中读取数据。消费者可以消费多个topic中的数据。 kafka 提供了两套 consumer API：  1. The high-level Consumer API 2. The SimpleConsumer API  high-level consumer API 提供了一个从 kafka 消费数据的高层抽象，而 SimpleConsumer API 则 需要开发人员更多地关注细节。    Consumer Group  每个Consumer属于一个特定的Consumer Group（可为每个Consumer指定group name，若不 指定group name则属于默认的group）。 将多个消费者集中到一起去处理某一个Topic的数据，可以更快的提高数据的消费能力 整个消费者组共享一组偏移量(防止数据被重复读取)，因为一个Topic有多个分区    offset偏移量  可以唯一的标识一条消息 偏移量决定读取数据的位置，不会有线程安全的问题，消费者通过偏移量来决定下次读取的消息 消息被消费之后，并不被马上删除，这样多个业务就可以重复使用kafka的消息 我们某一个业务也可以通过修改偏移量达到重新读取消息的目的,偏移量由用户控制 消息最终还是会被删除的，默认生命周期为1周（7*24小时）  Zookeeper  kafka 通过 zookeeper 来存储集群的 meta 信息。      "});index.add({'id':34,'href':'/docs/algorithm/%E5%9B%BE/4.%E5%9B%BE%E7%9A%84%E5%B9%BF%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95/','title':"4.「图」的广度优先搜索算法",'content':"4.「图」的广度优先搜索算法 既然我们之前已经提及了「深度优先搜索」算法了，那么作为它的兄弟算法「广度优先遍历」算法，我们就不得不提了。「广度优先搜索」算法不仅可以遍历「图」的所有顶点，也可以遍历两个顶点的所有路径。但是，「广度优先搜索」最高效的用途是：当在 权重相等且均为正数的「图」 中，它可以快速的找到两点之间的最短路径。\n虽然「深度优先搜索」算法也可以针对权重相等均且为正数的「图」找出两点之间的最短路径，但它需要先找出两点之间的所有路径之后，才可以求出最短路径。但是对于「广度优先搜索」，在大多数情况下，它可以不用找出所有路径，就能取出两点之间的最短路径。除非，最短路径出现在最后一条遍历的路径上，这种情况下，「广度优先搜索」也是遍历出了所有路径后，才取出的最短路径。\n如「图7. 无向图」所示，它的所有顶点分别为[A, C, D, B, E]。给定顶点 A 和 B， 它们之间有两条路径，一条路径为[A, C, D, B]，另一条路径为[A, E, B]。其中，[A, E, B]是顶点 A 和 B 之间的最短路径。\n 图7.无向图    「广度优先遍历」（又称「Breath First Search」，简称「BFS」）算法在「图」中主要用途：\n 遍历「图」中所有顶点； 针对 权重相等且均为正数的「图」，快速找出两点之间的最短路径。  "});index.add({'id':35,'href':'/docs/go/4.%E5%AD%97%E7%AC%A6%E4%B8%B2/','title':"4.字符串",'content':"4.字符串 "});index.add({'id':36,'href':'/docs/algorithm/%E5%9B%BE/2.%E5%B9%B6%E6%9F%A5%E9%9B%86/2.5.%E5%9F%BA%E4%BA%8E%E8%B7%AF%E5%BE%84%E5%8E%8B%E7%BC%A9%E7%9A%84%E6%8C%89%E7%A7%A9%E5%90%88%E5%B9%B6%E4%BC%98%E5%8C%96%E7%9A%84%E5%B9%B6%E6%9F%A5%E9%9B%86/','title':"2.5.基于路径压缩的按秩合并优化的「并查集」",'content':"2.5.基于路径压缩的按秩合并优化的「并查集」 这个优化就是将「路径压缩优化」和「按秩合并优化」合并后形成的「并查集」的实现方式。\n伪代码 // UnionFind.class public class UnionFind { int root[]; // 添加了 rank 数组来记录每个顶点的高度，也就是每个顶点的「秩」 int rank[]; public UnionFind(int size) { root = new int[size]; rank = new int[size]; for (int i = 0; i \u0026lt; size; i++) { root[i] = i; rank[i] = 1; // 一开始每个顶点的初始「秩」为1，因为它们只有自己本身的一个顶点。 } } // 此处的 find 函数与路径压优化缩版本的 find 函数一样。 public int find(int x) { if (x == root[x]) { return x; } return root[x] = find(root[x]); } // 按秩合并优化的 union 函数 public void union(int x, int y) { int rootX = find(x); int rootY = find(y); if (rootX != rootY) { if (rank[rootX] \u0026gt; rank[rootY]) { root[rootY] = rootX; } else if (rank[rootX] \u0026lt; rank[rootY]) { root[rootX] = rootY; } else { root[rootY] = rootX; rank[rootX] += 1; } } }; public boolean connected(int x, int y) { return find(x) == find(y); } } // App.java // 测试样例 public class App { public static void main(String[] args) throws Exception { UnionFind uf = new UnionFind(10); // 1-2-5-6-7 3-8-9 4 uf.union(1, 2); uf.union(2, 5); uf.union(5, 6); uf.union(6, 7); uf.union(3, 8); uf.union(8, 9); System.out.println(uf.connected(1, 5)); // true System.out.println(uf.connected(5, 7)); // true System.out.println(uf.connected(4, 9)); // false // 1-2-5-6-7 3-8-9-4 uf.union(9, 4); System.out.println(uf.connected(4, 9)); // true } } 时间复杂度     UnionFind 构造函数 find 函数 union 函数 connected 函数     时间复杂度 O(N) O(⍺(N)) O(⍺(N)) O(⍺(N))    注：N 为「图」中顶点的个数。\n"});index.add({'id':37,'href':'/docs/kafka/5.kafka%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/','title':"5.kafka环境搭建",'content':"5.kafka环境搭建 "});index.add({'id':38,'href':'/docs/go/5.%E5%87%BD%E6%95%B0/','title':"5.函数",'content':"5.函数 函数是一等公民  可以有多个返回值 所有参数都是值传递: slice, map, channel 会有传引用的错觉 函数可以作为变量的值 函数可以作为参数和返回值  可变参数 func Sum(ops ...int) int { ret := 0; for _, op:= range ops { ret += op } return ret } defer延迟运行 "});index.add({'id':39,'href':'/docs/algorithm/%E5%9B%BE/5.%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%E7%9B%B8%E5%85%B3%E7%AE%97%E6%B3%95/','title':"5.最小生成树相关算法",'content':"5.最小生成树相关算法 "});index.add({'id':40,'href':'/docs/algorithm/%E5%9B%BE/2.%E5%B9%B6%E6%9F%A5%E9%9B%86/2.6.%E5%B9%B6%E6%9F%A5%E9%9B%86%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E6%80%BB%E7%BB%93/','title':"2.6.「并查集」数据结构总结",'content':"2.6.「并查集」数据结构总结 在「并查集」数据结构中，其中心思想是将所有连接的顶点，无论是直接连接还是间接连接，都将他们指向同一个父节点或者根节点。此时，如果要判断两个顶点是否具有连通性，只要判断它们的根节点是否为同一个节点即可。\n在「并查集」数据结构中，它的两个灵魂函数，分别是 find和 union。find 函数是为了找出给定顶点的根节点。 union 函数是通过更改顶点根节点的方式，将两个原本不相连接的顶点表示为两个连接的顶点。对于「并查集」来说，它还有一个重要的功能性函数 connected。它最主要的作用就是检查两个顶点的「连通性」。find 和 union 函数是「并查集」中必不可少的函数。connected 函数则需要根据题目的意思来决定是否需要。\n「并查集」代码基本结构 public class UnionFind { // UnionFind 的构造函数，size 为 root 数组的长度 public UnionFind(int size) {} public int find(int x) {} public void union(int x, int y) {} public boolean connected(int x, int y) {} } 「并查集」的 find 函数 它主要是用于查找顶点 x 的根结点。\n find 函数的基本实现  public int find(int x) { while (x != root[x]) { x = root[x]; } return x; }  find 函数的优化 - 路径压缩  public int find(int x) { if (x == root[x]) { return x; } return root[x] = find(root[x]); } 「并查集」的 union 函数 它主要是连接两个顶点 x 和 y 。将它们的根结点变成相同的，即代表它们来自于同一个根节点。\n union 函数的基本实现  public void union(int x, int y) { int rootX = find(x); int rootY = find(y); if (rootX != rootY) { root[rootY] = x; } };  union 函数的优化 - 按秩合并  public void union(int x, int y) { int rootX = find(x); int rootY = find(y); if (rootX != rootY) { if (rank[rootX] \u0026gt; rank[rootY]) { root[rootY] = rootX; } else if (rank[rootX] \u0026lt; rank[rootY]) { root[rootX] = rootY; } else { root[rootY] = rootX; rank[rootX] += 1; } } }; 「并查集」的 connected 函数 它主要是检查两个顶点 x 和 y 的「连通性」。这个函数通过顶点 x 和 y 的根结点是否相同来判断 x 和 y 的「连通性」。如果 x 和 y 的根结点相同，则为连通。反之，则为不连通。\npublic boolean connected(int x, int y) { return find(x) == find(y); } 「并查集」的刷题小技巧 「并查集」的代码是高度模版化的。所以作者建议大家熟记「并查集」的实现代码，这样小伙伴们在遇到「并查集」的算法题目的时候，就可以淡定的应对了。作者推荐大家在理解的前题下，请熟记「基于路径压缩+按秩合并的并查集」的实现代码。\n"});index.add({'id':41,'href':'/docs/kafka/6.kafka%E6%95%B0%E6%8D%AE%E6%A3%80%E7%B4%A2%E6%9C%BA%E5%88%B6/','title':"6.kafka数据检索机制",'content':"6.kafka数据检索机制   topic在物理层面以partition为分组，一个topic可以分成若干个partition partition还可以细分为Segment，一个partition物理上由多个Segment组成  segment 的参数有两个：  log.segment.bytes：单个segment可容纳的最大数据量，默认为1GB log.segment.ms：Kafka在commit一个未写满的segment前，所等待的时间（默认为7 天）     LogSegment 文件由两部分组成，分别为“.index”文件和“.log”文件，分别表示为 Segment 索引文 件和数据文件。  partition全局的第一个segment从0开始，后续每个segment文件名为上一个segment文件 最后一条消息的offset值 数值大小为64位，20位数字字符长度，没有数字用0填充 第一个segment 00000000000000000000.index 00000000000000000000.log 第二个segment，文件命名以第一个segment的最后一条消息的offset组成 00000000000000170410.index 00000000000000170410.log 第三个segment，文件命名以上一个segment的最后一条消息的offset组成 00000000000000239430.index 00000000000000239430.log    消息都具有固定的物理结构，包括：offset(8 Bytes)、消息体的大小(4 Bytes)、crc32(4 Bytes)、 magic(1 Byte)、attributes(1 Byte)、key length(4 Bytes)、key(K Bytes)、payload(N Bytes)等等 字段，可以确定一条消息的大小，即读取到哪里截止。    "});index.add({'id':42,'href':'/docs/algorithm/%E5%9B%BE/6.%E5%8D%95%E6%BA%90%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%E7%9B%B8%E5%85%B3%E7%AE%97%E6%B3%95/','title':"6.单源最短路径相关算法",'content':"6.单源最短路径相关算法 "});index.add({'id':43,'href':'/docs/go/6.%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B/','title':"6.面向对象编程",'content':"6.面向对象编程 "});index.add({'id':44,'href':'/docs/algorithm/%E5%9B%BE/7.%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F%E4%B9%8Bkahn%E7%AE%97%E6%B3%95/','title':"7.拓扑排序之Kahn算法",'content':"7.拓扑排序之Kahn算法 "});index.add({'id':45,'href':'/docs/kafka/7.%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AE%89%E5%85%A8%E6%80%A7/','title':"7.数据的安全性",'content':"7.数据的安全性 producer delivery guarantee 0. At least one 消息绝不会丢，但可能会重复传输 1. At most once 消息可能会丢，但绝不会重复传输 2. Exactly once 每条消息肯定会被传输一次且仅传输一次  Producers可以选择是否为数据的写入接收ack，有以下几种ack的选项：request.required.acks  acks=0 * Producer 在 ISR 中的 Leader 已成功收到的数据并得到确认后发送下一条 Message。 acks=1 * 这意味着 Producer 无需等待来自 Broker 的确认而继续发送下一批消息。 acks=all * Producer 需要等待 ISR 中的所有 Follower 都确认接收到数据后才算一次发送完成，可 靠性最高。    ISR机制  关键词  AR : Assigned Replicas 用来标识副本的全集 OSR ： out -sync Replicas 离开同步队列的副本 ISR ： in -sync Replicas 加入同步队列的副本 ISR = Leader + 没有落后太多的副本;AR = OSR+ ISR。   我们备份数据就是防止数据丢失，当主节点挂掉时，可以启用备份节点  producer\u0026ndash;push\u0026ndash;\u0026gt;leader leader\u0026ndash;pull\u0026ndash;\u0026gt;follower Follower每间隔一定时间去Leader拉取数据，来保证数据的同步   ISR(in-syncReplica  当主节点挂点，并不是去Follower选择主，而是从ISR中选择主 判断标准  超过10秒钟没有同步数据  replica.lag.time.max.ms=10000   主副节点差4000条数据  rerplica.lag.max.messages=4000     脏节点选举  kafka采用一种降级措施来处理： 选举第一个恢复的node作为leader提供服务，以它的数据为基准，这个措施被称为脏 leader选举      broker数据存储机制  无论消息是否被消费，kafka 都会保留所有消息。有两种策略可以删除旧数据：  1. 基于时间：log.retention.hours=168 2. 基于大小：log.retention.bytes=1073741824 "});index.add({'id':46,'href':'/docs/go/7.%E7%BC%96%E5%86%99%E5%A5%BD%E7%9A%84%E9%94%99%E8%AF%AF%E5%A4%84%E7%90%86/','title':"7.编写好的错误处理",'content':"7.编写好的错误处理 "});index.add({'id':47,'href':'/docs/docker/','title':"docker",'content':"Docker 概念  镜像：可以理解为软件安装包，可以方便的进行传播和安装。 容器：软件安装后的状态，每个软件运行环境都是独立的、隔离的，称之为容器。  "});index.add({'id':48,'href':'/docs/go/2.%E5%9F%BA%E6%9C%AC%E7%A8%8B%E5%BA%8F%E7%BB%93%E6%9E%84/2.1.%E5%8F%98%E9%87%8Fvs%E5%B8%B8%E9%87%8F/','title':"2.1.变量vs常量",'content':"2.1.变量vs常量 变量赋值  赋值可以进行自动类型推断 在一个赋值语句中可以对多个变量进行同时赋值  a, b = b, a 常量定义 const ( Monday = iota + 1 Tuesday Wednesday ) const ( Readable = 1 \u0026lt;\u0026lt; iota Writable Executable ) "});index.add({'id':49,'href':'/docs/go/2.%E5%9F%BA%E6%9C%AC%E7%A8%8B%E5%BA%8F%E7%BB%93%E6%9E%84/2.2.%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/','title':"2.2.数据类型",'content':"2.2.数据类型    基本数据类型     bool   string   int int8 int16 int32 int64   uint uint8 uint16 uint32 uint64 uintptr   byte // alias for uint8   rune // alias for int32, represents a Unicode code point   float32 float64   complex64 complex128    ** go 不支持隐式类型转换,哪怕别名和原始类型都不能转换\n//机器位数 cpu := runtime.GOARCH //int占用位数 int_size := strconv.IntSize 类型的预定义值    math.MaxInt64    math.MaxFloat64    math.MaxUint32    指针类型  不支持指针运算 string 是值类型,其默认的初始化值为空字符串,而不是nil  "});index.add({'id':50,'href':'/docs/go/2.%E5%9F%BA%E6%9C%AC%E7%A8%8B%E5%BA%8F%E7%BB%93%E6%9E%84/2.3.%E8%BF%90%E7%AE%97%E7%AC%A6/','title':"2.3.运算符",'content':"2.3.运算符 \u0026amp;^ 按位置零\n1 \u0026amp;^ 0 1\n"});index.add({'id':51,'href':'/docs/go/2.%E5%9F%BA%E6%9C%AC%E7%A8%8B%E5%BA%8F%E7%BB%93%E6%9E%84/2.4.%E6%9D%A1%E4%BB%B6%E5%92%8C%E5%BE%AA%E7%8E%AF/','title':"2.4.条件和循环",'content':"2.4.条件和循环 循环 go 语言仅支持循环关键字for\nfor j := 7; j \u0026lt;= 9; j++\nwhile while条件循环\n# while (n\u0026lt;5) n := 0 for n \u0026lt; 5 { n++ fmt.Println(n) } 无限循环\n# while(true) n := 0 for { } if条件 if condition { } else { } if condition-1 { } else if condition-2 { } else { }    condition 表达式结果必须为布尔值    支持变量赋值    if var declaration; condition { // code to be executed if condiion is true } switch条件  1.条件表达式不限制为常量或者整数 2.单个case中,可以出现多个结果选项，使用逗号分隔 3.与C语言等规则相反，Go语言不需要用break来明确退出一个case 4.可以不设定switch之后的条件表达式，在此种情况下，整个switch结构与多个if\u0026hellip;else\u0026hellip;的逻辑作用等同  switch os := runtime.GOOS; os { case \u0026quot;drawin\u0026quot;: fmt.Println(\u0026quot;OS x.\u0026quot;) case \u0026quot;linux\u0026quot;: fmt.Println(\u0026quot;Linux.\u0026quot;) default: fmt.Printf(\u0026quot;%s.\u0026quot;, os) } switch { } "});index.add({'id':52,'href':'/docs/go/3.%E5%B8%B8%E7%94%A8%E9%9B%86%E5%90%88/3.1.%E6%95%B0%E7%BB%84%E5%92%8C%E5%88%87%E7%89%87/','title':"3.1.数组和切片",'content':"3.1.数组和切片 数组  数组的声明  var a [3]int // 声明并初始化为默认零值 a[0] = 1 b := [3]int{1, 2, 3} // 声明同时初始化 c := [2][2]int{{1, 2}, {3, 4}} // 多维数组初始化  数组元素遍历  func TestTravelArray(t *testing.T) { a := [...]int{1, 2, 3, 4, 5} // 不指定元素个数 for idx/*索引*/, elem/*元素*/ := range a { fmt.Println(idx, elem) } }  数组截取 a[开始索引(包含), 结束索引(不包含)]  a := [...]int{1, 2, 3, 4, 5} a[1:2] //2 a[1:3] //2,3 a[1:len(a)] // 2, 3, 4, 5 a[1:] // 2, 3, 4, 5 a[:3] // 1, 2, 3 切片   切片内部结构    切片声明\n  var s0 []int s0 = append(s0, 1) s := []int{} s1 := []int{1,2,3,4} // 定义 s2 := make([]int, 3, 5) /* []type, len, cap 其中len个元素会被初始化为默认零值，未初始化元素不可以访问 */  切片共享存储结构    数组vs切片    容量是否可伸缩 数组不可伸缩，切片可伸缩    是否可以进行比较 数组维度相同长度的可以比较    "});index.add({'id':53,'href':'/docs/go/3.%E5%B8%B8%E7%94%A8%E9%9B%86%E5%90%88/3.2.map%E5%A3%B0%E6%98%8E%E5%85%83%E7%B4%A0%E8%AE%BF%E9%97%AE%E5%8F%8A%E9%81%8D%E5%8E%86/','title':"3.2.map声明元素访问及便利",'content':"3.2.map声明元素访问及便利  声明  m := map[string]int{\u0026quot;one\u0026quot;: 1, \u0026quot;two\u0026quot;: 2, \u0026quot;three\u0026quot;: 3} m1 := map[string]int{} m1[\u0026quot;one\u0026quot;] = 1 m2 := make(map[String]int, 10 /*Initial Capacity*/)  Map元素的访问 在访问的key不存在时,仍会返回零值,不能通过返回nil来判断元素是否存在  if v, ok := m1[3]; ok { t.Logf(\u0026quot;key 3 is %d\u0026quot;, v) } else { t.Log(\u0026quot;key 3 is existing\u0026quot;) }  Map遍历  "});index.add({'id':54,'href':'/docs/go/3.%E5%B8%B8%E7%94%A8%E9%9B%86%E5%90%88/3.3.map%E4%B8%8E%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F%E5%9C%A8go%E8%AF%AD%E8%A8%80%E4%B8%AD%E5%AE%9E%E7%8E%B0set/','title':"3.3.Map与工厂模式,在go语言中实现set",'content':"3.3.Map与工厂模式,在go语言中实现set Map与工厂模式  Map的value可以是一个方法 与Go的Dock type接口方式一起,可以方便的实现单一方法对象的工厂模式  实现set go的内置集合中没有set实现, 可以map[type]bool\n 1.元素的唯一性 2.基本操作  1.添加元素 2.判断元素是否存在 3.删除元素 4.元素个数    "});index.add({'id':55,'href':'/docs/go/6.%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B/6.1.%E6%8E%A5%E5%8F%A3%E7%9B%B8%E5%85%B3/','title':"6.1.接口相关",'content':"6.1.接口相关  接口为非入侵性,实现不依赖于接口定义 所以接口的定义可以包含在接口使用者包内  "});index.add({'id':56,'href':'/docs/go/6.%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B/6.3.%E6%89%A9%E5%B1%95%E4%B8%8E%E5%A4%8D%E7%94%A8/','title':"6.3.扩展与复用",'content':"6.3.扩展与复用 "});index.add({'id':57,'href':'/docs/go/7.%E7%BC%96%E5%86%99%E5%A5%BD%E7%9A%84%E9%94%99%E8%AF%AF%E5%A4%84%E7%90%86/7.1.%E7%BC%96%E5%86%99%E5%A5%BD%E7%9A%84%E9%94%99%E8%AF%AF%E5%A4%84%E7%90%86/','title':"7.1.编写好的错误处理",'content':"7.1.编写好的错误处理 Go的错误机制  没有异常机制 error类型实现了error接口 可以通过errors.New来快速创建错误实例  "});index.add({'id':58,'href':'/docs/go/7.%E7%BC%96%E5%86%99%E5%A5%BD%E7%9A%84%E9%94%99%E8%AF%AF%E5%A4%84%E7%90%86/7.2.panic%E5%92%8Crecover/','title':"7.2.panic和",'content':"7.1.编写好的错误处理recover panic  panic用于不可以恢复的错误 panic退出前会执行defer指定的内容  "});index.add({'id':59,'href':'/docs/vbox/','title':"vbox",'content':"vbox vbox常用到的一些配置\n"});index.add({'id':60,'href':'/docs/algorithm/%E5%9B%BE/5.%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%E7%9B%B8%E5%85%B3%E7%AE%97%E6%B3%95/5.1.%E5%88%87%E5%88%86%E5%AE%9A%E7%90%86/','title':"5.1.切分定理",'content':""});index.add({'id':61,'href':'/docs/algorithm/%E5%9B%BE/5.%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%E7%9B%B8%E5%85%B3%E7%AE%97%E6%B3%95/5.2.kruskal%E7%AE%97%E6%B3%95/','title':"5.2. Kruskal算法",'content':""});index.add({'id':62,'href':'/docs/algorithm/%E5%9B%BE/5.%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%E7%9B%B8%E5%85%B3%E7%AE%97%E6%B3%95/5.3.prim%E7%AE%97%E6%B3%95/','title':"5.3. Prim算法",'content':""});index.add({'id':63,'href':'/docs/go/%E5%AE%89%E8%A3%85/','title':"安装",'content':"安装 卸载docker sudo yum remove docker\ndocker-client\ndocker-client-latest\ndocker-common\ndocker-latest\ndocker-latest-logrotate\ndocker-logrotate\ndocker-engine\n安装docker环境依赖 yum install -y yum-utils device-mapper-persistent-data lvm2\n配置国内docker-ce的yum源(阿里云) yum-config-manager \u0026ndash;add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n会下载 /etc/yum.repos.d/docker-ce.repo 文件\n安装docker yum install aocker-ce docker-ce-cli containerd.io -y\ndocker-ce-cli docker命令行工具包 containerd.io 容器接口相关包 启动并设置开机启动 systemctl start docker \u0026amp;\u0026amp; systemctl enable docker\ncurl -fsSL get.docker.com -o get-docker.sh\n"});index.add({'id':64,'href':'/docs/go/%E5%B9%B6%E5%8F%91/%E5%9F%BA%E6%9C%AC%E5%B9%B6%E5%8F%91%E5%8E%9F%E8%AF%AD/%E4%BA%92%E6%96%A5%E9%94%81%E7%9A%84%E5%AE%9E%E7%8E%B0%E6%9C%BA%E5%88%B6/','title':"互斥锁的实现机制",'content':"互斥锁的实现机制 临界区 在并发编程中，如果程序中的一部分会被并发访问或修改，那么，为了避免并发访问导致的意想不到的结果，这部分程序需要被保护起来，这部分被保护起来的程序，就叫做临界区。\n我们可以使用互斥锁，限定临界区只能同时由一个线程持有。\n当临界区由一个线程持有的时候，其它线程如果想进入这个临界区，就会返回失败，或者是等待。直到持有的线程退出临界区，这些等待线程中的某一个才有机会接着持有这个临界区。\nMutex 是使用最广泛的同步原语（Synchronization primitives，有人也叫做并发原语。\n在这门课的前两个模块，我会和你讲互斥锁 Mutex、读写锁 RWMutex、并发编排 WaitGroup、条件变量 Cond、Channel 等同步原语。所以，在这里，我先和你说一下同步原语的适用场景。\n同步原语的适用场景\n 共享资源。并发地读写共享资源，会出现数据竞争（data race）的问题，所以需要 Mutex、RWMutex 这样的并发原语来保护。(共享资源一般靠锁来控制) 任务编排。需要 goroutine 按照一定的规律执行，而 goroutine 之间有相互等待或者依赖的顺序关系，我们常常使用 WaitGroup 或者 Channel 来实现。 消息传递。信息交流以及不同的 goroutine 之间的线程安全的数据交流，常常使用 Channel 来实现。  在 Go 的标准库中，package sync 提供了锁相关的一系列同步原语，这个 package 还定义了一个 Locker 的接口，Mutex 就实现了这个接口。\n简单来说，互斥锁 Mutex 就提供两个方法 Lock 和 Unlock：进入临界区之前调用 Lock 方法，退出临界区的时候调用 Unlock 方法：\n当一个 goroutine 通过调用 Lock 方法获得了这个锁的拥有权后， 其它请求锁的 goroutine 就会阻塞在 Lock 方法的调用上，直到锁被释放并且自己获取到了这个锁的拥有权。\nimport ( \u0026quot;fmt\u0026quot; \u0026quot;sync\u0026quot; ) func main() { var count = 0 // 使用WaitGroup等待10个goroutine完成 var wg sync.WaitGroup wg.Add(10) for i := 0; i \u0026lt; 10; i++ { go func() { defer wg.Done() // 对变量count执行10次加1 for j := 0; j \u0026lt; 100000; j++ { count++ } }() } // 等待10个goroutine完成 wg.Wait() fmt.Println(count) } 针对这个问题，Go 提供了一个检测并发访问共享资源是否有问题的工具： race detector，它可以帮助我们自动发现程序有没有 data race 的问题。\n在编译（compile）、测试（test）或者运行（run）Go 代码的时候，加上 race 参数，就有可能发现并发问题。比如在上面的例子中，我们可以加上 race 参数运行，检测一下是不是有并发问题。如果你 go run -race counter.go，就会输出警告信息。\n在编译的代码中，增加了 runtime.racefuncenter、runtime.raceread、runtime.racewrite、runtime.racefuncexit 等检测 data race 的方法。通过这些插入的指令，Go race detector 工具就能够成功地检测出 data race 问题了。\nhttps://golang.org/src/sync/mutex.go 中注释Mutex fairness.\n"});index.add({'id':65,'href':'/docs/redis/%E7%BC%93%E5%AD%98/%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F/','title':"缓存穿透",'content':""});index.add({'id':66,'href':'/posts/','title':"Posts",'content':""});index.add({'id':67,'href':'/docs/','title':"Docs",'content':""});index.add({'id':68,'href':'/docs/notices/','title':"成\"神\"之路",'content':""});})();